{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install praw\n!pip install transformers torch\n!pip install datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T19:19:47.472678Z","iopub.execute_input":"2025-02-09T19:19:47.472936Z","iopub.status.idle":"2025-02-09T19:20:06.708657Z","shell.execute_reply.started":"2025-02-09T19:19:47.472912Z","shell.execute_reply":"2025-02-09T19:20:06.707548Z"}},"outputs":[{"name":"stdout","text":"Collecting praw\n  Downloading praw-7.8.1-py3-none-any.whl.metadata (9.4 kB)\nCollecting prawcore<3,>=2.4 (from praw)\n  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\nRequirement already satisfied: update_checker>=0.18 in /usr/local/lib/python3.10/dist-packages (from praw) (0.18.0)\nRequirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw) (1.8.0)\nRequirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from prawcore<3,>=2.4->praw) (2.32.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2025.1.31)\nDownloading praw-7.8.1-py3-none-any.whl (189 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.3/189.3 kB\u001b[0m \u001b[31m206.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading prawcore-2.4.0-py3-none-any.whl (17 kB)\nInstalling collected packages: prawcore, praw\nSuccessfully installed praw-7.8.1 prawcore-2.4.0\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.28.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.17.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.11)\nRequirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.28.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from datasets import load_dataset\n\nds = load_dataset(\"ccdv/arxiv-summarization\", \"document\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T19:27:45.856991Z","iopub.execute_input":"2025-02-09T19:27:45.857360Z","iopub.status.idle":"2025-02-09T19:27:47.719771Z","shell.execute_reply.started":"2025-02-09T19:27:45.857331Z","shell.execute_reply":"2025-02-09T19:27:47.718939Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from transformers import BartTokenizer, BartForConditionalGeneration, Trainer, TrainingArguments\nimport torch\n\n# Check for GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Tokenizer\ntokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n\n# Preprocess Function\ndef preprocess_function(examples):\n    inputs = [doc for doc in examples[\"article\"]]\n    model_inputs = tokenizer(inputs, max_length=1024, truncation=True, padding=\"max_length\")\n    \n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(examples[\"abstract\"], max_length=128, truncation=True, padding=\"max_length\")\n    \n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n\n# Apply preprocessing on only 10,000 samples\nds_small = ds[\"train\"].select(range(10000))  # Select first 10k samples\ntokenized_ds = ds_small.map(preprocess_function, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T19:27:49.860301Z","iopub.execute_input":"2025-02-09T19:27:49.860739Z","iopub.status.idle":"2025-02-09T19:36:52.902318Z","shell.execute_reply.started":"2025-02-09T19:27:49.860702Z","shell.execute_reply":"2025-02-09T19:36:52.901394Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7812276022c4b609e86dc078b8f057c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9a8c84867d441bfbc0defed57ddcc94"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c07cb0695f646cb9660791b6a1e2bf6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba968949b0b44715943f8fb5326c13d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae7571b166ec4c23817902bcddd65d94"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\").to(device)\n# Training Arguments (removed 'device' argument)\ntraining_args = TrainingArguments(\n    output_dir=\"./fine-tuned-model\",\n    eval_strategy=\"epoch\",               # Updated here\n    learning_rate=2e-5,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    save_total_limit=2,\n    save_steps=500,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    report_to=\"none\",                   # Disable W&B\n)\n\n\n# Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_ds,  # Use the 10k subset\n    eval_dataset=tokenized_ds,   # Optional: same for validation if separate validation isn't available\n)\n\n# Fine-tune the model\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T19:37:28.010156Z","iopub.execute_input":"2025-02-09T19:37:28.010584Z","iopub.status.idle":"2025-02-09T22:50:01.030652Z","shell.execute_reply.started":"2025-02-09T19:37:28.010550Z","shell.execute_reply":"2025-02-09T22:50:01.029734Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04b5e2f644fd41d79c056acba180da16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82dac7a8a8a94ea68bb7b6cebbb15ff9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='7500' max='7500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [7500/7500 3:12:20, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.291700</td>\n      <td>2.010976</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.974400</td>\n      <td>1.797372</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.846800</td>\n      <td>1.703413</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=7500, training_loss=2.1029660046895344, metrics={'train_runtime': 11542.4153, 'train_samples_per_second': 2.599, 'train_steps_per_second': 0.65, 'total_flos': 6.501313806336e+16, 'train_loss': 2.1029660046895344, 'epoch': 3.0})"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n\nmodel.save_pretrained(\"saved_model\")\ntokenizer_ds.save_pretrained(\"saved_model\")\n\n# Load the saved model and tokenizer\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"saved_model\")\ntokenizer = AutoTokenizer.from_pretrained(\"saved_model\")\n\n# Initialize the summarizer pipeline with the loaded model and tokenizer\nsummarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, device=0 if device == \"cuda\" else -1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Replace with your Reddit API credentials\nCLIENT_ID = \"G6R7wy_ArTl2L3zuqFB8sA\"\nCLIENT_SECRET = \"H1Hv2Ys46-wkgJmD4B6_gL-3-ACGHw\"\nUSER_AGENT = \"Flimsy_Mouse_622\"\n\n# Authenticate\nreddit = praw.Reddit(\n    client_id=CLIENT_ID,\n    client_secret=CLIENT_SECRET,\n    user_agent=USER_AGENT\n)\n\n\n# Fetch the latest posts from a subreddit\ndef fetch_posts(subreddit_name, limit=5):\n    subreddit = reddit.subreddit(subreddit_name)\n    posts = []\n    for post in subreddit.new(limit=limit):\n        posts.append({\n            \"title\": post.title,\n            \"body\": post.selftext,\n            \"url\": post.url\n        })\n    return posts\n\nimport re\n\ndef clean_text(text):\n    \"\"\"\n    Clean the text by removing special characters, URLs, and extra spaces.\n    \"\"\"\n    # Remove URLs\n    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n    # Remove special characters (keep only alphanumeric and basic punctuation)\n    text = re.sub(r\"[^a-zA-Z0-9\\s.,!?]\", \"\", text)\n    # Remove extra spaces\n    text = \" \".join(text.split())\n    return text\n\ndef summarize_post(post):\n    \"\"\"\n    Summarize a post's title and body.\n    \"\"\"\n    text = f\"{post['title']}. {post['body']}\"\n    text = clean_text(text)  # Clean the text\n    print(f\"Text to summarize: {text}\")  # Debug print\n\n    # Skip summarization if the text is too short or too long\n    if len(text.split()) < 5:  # Skip if the text is too short\n        print(\"Skipping summarization: Text is too short.\")\n        return post[\"title\"]  # Return the title as a fallback\n    if len(text.split()) > 1024:  # Truncate if the text is too long\n        print(\"Truncating text: Text is too long.\")\n        text = \" \".join(text.split()[:1024])\n\n    # Summarize the text\n    try:\n        summary = summarizer(text, max_length=50, min_length=25, do_sample=False)\n        return summary[0]['summary_text']\n    except Exception as e:\n        print(f\"Error summarizing text: {e}\")\n        return post[\"title\"]  # Return the title as a fallback\n\n# Summarize multiple posts\ndef summarize_posts(posts):\n    \"\"\"\n    Summarize a list of posts.\n    \"\"\"\n    summaries = []\n    for post in posts:\n        summary = summarize_post(post)\n        summaries.append({\n            \"title\": post[\"title\"],\n            \"summary\": summary,\n            \"url\": post[\"url\"]\n        })\n    return summaries\n\n# Example usage\nsubreddit_name = \"MachineLearning\"\nposts = fetch_posts(subreddit_name, limit=5)\nsummaries = summarize_posts(posts)\n\n# Print the results\nfor summary in summaries:\n    print(\"Title:\", summary[\"title\"])\n    print(\"Summary:\", summary[\"summary\"])\n    print(\"URL:\", summary[\"url\"])\n    print(\"---\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}